
<!DOCTYPE html>
<html lang="en">
<head>
    <title>Davide Valeriani - BCI Researcher</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/bootstrap-theme.min.css">
    <link rel="stylesheet" href="css/font.css" type='text/css'>
    <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon" />
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <div id="wrap">
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>

                <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                    <ul class="nav navbar-nav" role="navigation">
                        <li><a href="index.html"><span class="glyphicon glyphicon-home" aria-hidden="true"></span> Home</a></li>
                        <li class="active dropdown">
                        <a class="dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false"><span class="glyphicon glyphicon-star" aria-hidden="true"></span> Research <span class="caret"></span></a>
                            <ul class="dropdown-menu">
                                <li><a href="research.html#phd">Group Decision Making</a></li>
                                <li><a href="research.html#cybathlon">Cybathlon 2016</a></li>
                                <li><a href="research.html#eyewink">EyeWink</a></li>
                            </ul>
                        </li>
                        <li><a href="publications.html"><span class="glyphicon glyphicon-list-alt" aria-hidden="true"></span> Publications</a></li>
                        <li><a href="teaching.html"><span class="glyphicon glyphicon-blackboard" aria-hidden="true"></span> Teaching</a></li>
                        <li><a href="media.html"><span class="glyphicon glyphicon-bullhorn" aria-hidden="true"></span> Talks & Media</a></li>
                        <li><a href="notes.php"><span class="glyphicon glyphicon-pencil" aria-hidden="true"></span> Unipr</a></li>
                        <li><a href="more.html"><span class="glyphicon glyphicon-forward" aria-hidden="true"></span> More</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <div class="container">
            <h2>Research</h2>
            <div class="row">
                <div class="col-md-12">
                    <a name="phd" class="anchor"></a>
                    <h3>Improving group decision making with collaborative Brain-Computer Interfaces</h3>
                    <p>
                        My research is focused on using Brain-Computer Interfaces (BCIs) to improve group decision making. We are aware that group decisions are usually much better than the decisions made by an individual. This is why we usually make critical decisions in groups (e.g., committees, parliaments, etc.). Group decisions are the result of a process of integration of the multiple perspectives of its members, each of which brings a novel piece of information that might be useful for making the correct decision. This is what we call the <em>wisdom of crowds</em> (<a href="https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds" target="_blank">a good book about that</a>).</p>
                    <p>
                        Sometimes group decisions are not possible. For example, when we have time constraints that do not allow a complete discussion between the team members (that is a vital step of that integration process described above) or when a strong team leader can transform the group decision in an unilateral one. In these circumstances, group decisions could actually be worse than individual ones.
                    </p>
                    <p>
                        During my PhD, I investigated the possibility of using BCIs to extract neural information related to the confidence in a decision from the EEG signals of individual users. <strong>The BCI uses machine learning techniques to build an estimation of the confidence of each decision maker</strong> based on these neural correlates, response times (RTs) and other physiological measures (e.g., eye movements, skin conductance). These estimates are then used to weigh individual decisions proportionally and build a group decision. I call this <strong>collaborative BCIs</strong> (cBCIs) as multiple BCIs (i.e., one per user) are used jointly to achieve a goal (i.e., improve group decisions).
                    </p>
                    <p align="center"><img src="img/cBCI.jpg" width="90%" vspace="15px" /></p>
                    <p>
                        We firstly tested this approach with an easy <strong>visual matching task</strong> (see <a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0102693" target="_blank">this paper</a>) where
                        users were presented two displays containing three shapes of different shades of grey and had to decide if the two
                        stimuli contained the same shapes (picture below on the left).
                    </p>
                    <p>
                        Principal Component Analysis (PCA) has been used to extract neural features from the response-locked EEG epochs.
                        The figure below (right) shows the mean error rates of groups of different sizes where group decisions were obtained by using the standard majority rule (black) or a weighted majority where the weighs were wither the RTs (blue), the confidence estimated by the BCI using EEG features (green) and a combination of RTs and the BCI confidence (red).
                    </p>
                    <p align="center">
                        <img src="img/stim_plos.png" height="200px" vspace="15px" />
                        <img src="img/errors_plos.png" height="200px" vspace="15px" />
                    </p>
                    <p>
                        These promising results pushed me to further improve the system and put it on test with other tasks that were <em>more complex</em> and <em>realistic</em>.
                    </p>
                    <p>
                        Firstly, I tested this cBCI with a <strong>visual-search</strong> task using artificial stimuli (see <a href="pub/NER2015_1.pdf" target="_blank">this conference paper</a>, <a href="pub/TBME2016.pdf" target="_blank">this journal paper</a> and the stimulus below).
                        Here, the cBCI has also been improved by changing the method
                        used to extract neural features from the PCA used before to <strong>Common Spatial Pattern</strong> (CSP). This
                        reduced the dimensionality of the problem and speed up the system by two orders of magnitudes. Moreover, stimulus-locked EEG
                        epochs have been included in the analysis as they contain relevant information about the decision confidence that improved
                        the performance of BCI-assisted groups.
                    </p>
                    <p align="center">
                        <img src="img/stim_bars.png" height="200px" vspace="15px" />
                    </p>
                    <p>
                        A similar visual-search experiment has been conducted with <strong>realistic stimuli</strong>, where users were asked to identify polar bears in picture of
                        Arctic environments with lots of penguins (see <a href="pub/NER2015_2.pdf" target="_blank">this paper</a>).
                        Here, the information about the decision confidence was extracted from the neural signals, the RTs
                        and the <strong>eye movements</strong>. Eye movements and blinks, surprisingly, seem to correlate
                        with the decision confidence and their inclusion significantly increase the performance of the system.
                    </p>
                    <p>
                        Moving forward towards realistic applications, I have recently tested the performance of this cBCI with a <strong>face recognition</strong> task, where participants had to identify target people in a crowded scene taken from a security camera (see <a href="pub/NER2017.pdf" target="blank">this paper</a>). Once again, the performance of cBCI-assisted groups was significantly better than the performance of equally-sized traditional groups using simple majority. Moreover, I have also tested the possibility of using a <strong>confidence estimate reported by participants</strong> after each decision to weigh individual responses and obtain group decisions. For most group sizes, however, the cBCI was still providing much more accurate confidence estimates (i.e., more correlated with the correctness in a decision) than the reported confidence.
                    </p>
                    <p>
                        I have also tested the impact of interaction on group performance. Interestingly, when people are allowed to exchange information during the experiment, their performance degrades significantly. They also report confidence estimates that are uncorrelated with the objective accuracy. More details on our <a href="http://dx.doi.org/10.1038/s41598-017-08265-7" target="blank">Scientific Reports paper</a>.
                    </p>
                    <hr>
                    <a name="cybathlon" class="anchor"></a>
                    <h3>Cybathlon 2016: a Bronze medal to our team</h3>
                    <p>
                        I am a member of the <a href="http://essexbcis.uk/main/cybathlon/team/" target="_blank">Essex Brainstormers</a>, the student team from the University of Essex that competed in the <a href="http://www.cybathlon.ethz.ch/en/" target="_blank">Cybathlon 2016</a>. Cybathlon is the first international championship for parathletes assisted by technologies competing in six different disciplines, including BCI. In the BCI race, pilots had to control a videogame (3 commands available) with their mind. Our team ended up in the third position, bringing back a <strong>bronze medal</strong> to the UK (see the <a href="http://www.cybathlon.ethz.ch/en/cybathlon-news/cybathlon-results/bci-results.html" target="blank">final ranking</a>).
                    </p>
                    <p>
                        Our team developed a whole asynchronous BCI able to recognise different mental tasks from the EEG signals recorded from our pilot <a href="http://essexbcis.uk/main/cybathlon/pilot/" target="blank">David</a> and map them to the various commands supported by the game. The system is obviously very complex as it includes the development of various parts. One of the most difficult parts to implement was the multi-class classification. For this task, we developed a multilayer ensemble (see <a href="pub/ECDA2015.pdf">this abstract</a> and the relative <a href="lectures/2015/ecda2015.pdf">presentation</a>).
                    </p>
                    <p>
                        The BrainStormers were supported by <a href="http://www.biosemi.com/" target="blank">Biosemi</a>, by the University of Essex (via a Research Innovative Fund grant) and by a crowdfunding campaign with which we raised more than &pound;3,000.
                        The research conducted with the Brainstormers has attracted the interest of the media including <a href="http://www.itv.com/news/anglia/2016-05-18/essex-team-hope-to-storm-to-success-at-worlds-first-ever-cybathlon-competition/" target="_blank"><strong>ITV Anglia</strong></a> and <a href="https://cybathlon.wordpress.com/2015/01/29/brainstormers-and-bcis-on-your-marks-set-think/" target="_blank">Inside Cybathlon</a>.
                    </p>
                    <p align="center">
                        <img src="img/david_race.jpg" height="300px" />
                        <img src="img/brainstormers.jpg" height="300px" />
                    </p>
                    <hr>
                    <a name="eyewink" class="anchor"></a>
                    <h3>EyeWink</h3>
                    <img src="img/eyewink_logo.png" align="right" hspace="15px">
                    <p>
                        EyeWink is a concept of a wearable device that allows a user to control his smartphone with eye winks. It detects muscular activity around the eyes by means of two electrodes placed on the forehead. The recorded signals are then processed by an electronic board to identify if the user had winked with either eye (see <a href="pub/CEEC2015.pdf">this paper</a>). Then, winks are transformed in commands and sent to the smartphone to be executed. The user can select which command he/she wants to perform with either eye via a smartphone app.
                    </p>
                    <p>
                        After pitching the idea at <a href="http://hackthebrain.uk/" target="_blank">HackTheBrain UK</a>, my team developed a first fully-working prototype in only 8 hours using <a href="http://openbci.com/community/hack-the-brain-uk-control-your-smartphone-by-winking/" target="_blank">OpenBCI</a>. After winning the hackathon, I decided to keep working on the idea with Ana Matran-Fernandez to bring it to the market. In November 2015 we started a <a href="https://click.hubbub.net/p/eye-wink" target="_blank">crowdfunding campaign</a> that allowed us to raise &pound;4,605 to support the development, and in December 2015 I co-founded <a href="https://www.eyewink.net" target="_blank">EyeWink Ltd</a>.
                    </p>
                    <p>
                        EyeWink has attracted a lot of interest from the media and the public. We have presented the technology in the London Science Museum twice, once in April 2015 within the exhibition "You have been upgraded" and once in April 2016 for the Science Museum Lates focused on neuroscience. We have appeared on
                        <strong><a href="http://www.itv.com/news/anglia/2015-06-10/eye-phone-students-develop-new-tech-to-operate-mobiles/" target="_blank">ITV Anglia</a></strong>,
                        <a href="http://www.popsci.com/control-your-projects-with-your-mind" target="_blank">Popular Science</a>,
                        <a href="http://motherboard.vice.com/read/brain-hackathon-hacking-brainwaves-to-extend-the-mind" target="_blank">Motherboard magazine</a>,
                        <a href="http://www.idgconnect.com/blog-abstract/9663/the-uk-hack-brain-event" target="_blank">IDG connect blog</a>,
                        <a href="http://www.idgconnect.com/abstract/10691/crowdsourcing-innovation-davide-valeriani-ana-matran-fernandez-eyewink" target="_blank">IDG connect innovation</a>,
                        <a href="http://neurotechnews.com/?p=823" target="_blank">Neurotech news</a>,
                        <a href="http://www.christinethecoach.com/eyewink/" target="_blank">Christine Michaelis blog</a>,
                        <a href="https://www.essex.ac.uk/csee/news_and_seminars/newsEvent.aspx?e_id=7553" target="_blank">our department newsletter</a>,
                        the <a href="http://blogs.essex.ac.uk/essexdaily/2015/05/11/its-all-in-a-winkor-a-blink/" target="_blank">University of Essex daily news</a>,
                        <a href="http://www.nerri.eu/eng/news-highlights/nerri-news/hack-the-brain.aspx" target="_blank">NERRI project blog</a>,
                        some Italian newspapers including <a href="http://www.ilfattoquotidiano.it/2015/12/15/dottorato-a-londra-dopo-colloquio-via-skype-impensabile-in-italia-lavorare-al-mio-progetto-ultratecnologico/2302876/" target="_blank">Il Fatto Quotidiano</a>,
                        <a href="http://www.24emilia.com/Sezione.jsp?idSezione=68111" target="_blank">24Emilia</a>,
                        <a href="http://www.ilrestodelcarlino.it/reggio-emilia/eyewink-davide-valeriani-1.1537012" target="_blank">Resto del Carlino</a>,
                        <a href="http://gazzettadireggio.gelocal.it/reggio/cronaca/2015/11/24/news/lo-smartphone-lo-comandi-con-gli-occhi-1.12497579" target="_blank">Gazzetta di Reggio</a>,
                        and radios like <a href="https://www.youtube.com/watch?v=403Uc5LlKE8" target="_blank">Radio Citt&agrave; Aperta</a>.
                    </p>
                    <p>
                        In January 2017, EyeWink Ltd has joined the <a href="https://www.essex.ac.uk/business/knowledge-gateway/start-up-hub/default.aspx" target="blank">Start-up Hub</a> at the University of Essex to obtain hot-space desk and dedicated business support.
                    </p>
                    <p>
                        In June 2017, the <strong>University of Essex invested &pound;15,000</strong> on EyeWink Ltd through a convertible loan to support the transition from prototyping to the market.
                    </p>
                    <p align="center">
                        <img src="img/eyewink_lates.jpg" height="300px" />
                        <iframe width="533" height="300" src="https://www.youtube.com/embed/1fRnrFB8ndI" frameborder="0" allowfullscreen align="right"></iframe>
                    </p>


                </div>
            </div>
        </div>
        <div id="push"></div>
    </div>
    <div id="footer">
        <div class="container">
            <p class="muted credit">Copyright &copy; 2019 Davide Valeriani. All rights reserved.</p>
        </div>
    </div>

    <!-- Latest compiled and minified JavaScript -->
    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-55902637-1', 'auto');
      ga('send', 'pageview');
  </script>
</body>
</html>
