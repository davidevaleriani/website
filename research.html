
<!DOCTYPE html>
<html lang="en">
<head>
    <title>Davide Valeriani, PhD</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/bootstrap-theme.min.css">
    <link rel="stylesheet" href="css/font.css" type='text/css'>
    <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon" />
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <div id="wrap">
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>

                <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                    <ul class="nav navbar-nav" role="navigation">
                        <li><a href="index.html"><span class="glyphicon glyphicon-home" aria-hidden="true"></span> Home</a></li>
                        <li class="active"><a href="research.html"><span class="glyphicon glyphicon-search" aria-hidden="true"></span> Research</a></li>
                        <li><a href="publications.html"><span class="glyphicon glyphicon-book" aria-hidden="true"></span> Publications</a></li>
                        <li><a href="honors.html"><span class="glyphicon glyphicon-star" aria-hidden="true"></span> Honors</a></li>
                        <li><a href="teaching.html"><span class="glyphicon glyphicon-blackboard" aria-hidden="true"></span> Teaching</a></li>
                        <li><a href="outreach.html"><span class="glyphicon glyphicon-bullhorn" aria-hidden="true"></span> Outreach</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <div class="container">
            <h2>Research</h2>
             <p>My research focuses on developing neurotechnologies for human enhancement, using neuroimaging and machine learning. In particular, I merge brain and machines to achieve better decisions in critical domains. Below are a few examples.</p>
            <div class="row">
                <div class="col-md-12">
                    <a name="phd" class="anchor"></a>
                    <h3>Automatic Diagnostic Tools of Laryngeal Dystonia</h3>
                    <p>
                        Laryngeal dystonia (LD) is a neurologic disorder causing involuntary spasms in the laryngeal muscles, hence affecting our ability to speak. Its causes are unknown and there are no objective diagnostic criteria.
                    </p>
                    <p>
                        <img src="img/deep_dystonia.jpg" align="right" hspace="15px" vspace="10px" width="50%">
                        In my research, I use multimodal neuroimaging and machine learning to make automatic diagnosis of laryngeal dystonia. More specifically, I envision the neurologist of the future ordering a magnetic resonance (MR) scan, loading the image into a software, and obtaining an accurate diagnosis for the patient in less than a minute. The output of the software could be used as a final diagnosis, or as an additional opinion to help clinicians with their diagnosis.
                    </p>

                    <h3>Collaborative Brain-Computer Interfaces for Improving Group Decision Making</h3>
                    <p>
                        Groups are usually much better than individuals in making decisions (<a href="https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds" target="_blank">wisdom of crowds</a>). However, there are cases in which groups are not as accurate as expected, for example when composed of overconfident team members.</p>
                    <p>
                        <img src="img/cBCI.jpg" width="50%" align="right" hspace="15px" vspace="0px" />
                        My research goal is to develop neurotechnologies that could help groups make accurate decisions, regardless the personality of their team members. In particular, I have developed a brain-computer interface (BCI) able to estimate the confidence in a decision from the EEG signals of a human subject. If all group members are equipped with such a BCI, these confidence estimates could be used to weigh individual decisions and build a group decision. I call this <em>collaborative BCIs</em>, as multiple BCIs are used jointly to improve group decisions.
                    </p>
                    <p>
                        We tested this approach with a variety of tasks of increasing realism and complexity. We started from <a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0102693" target="_blank">visual matching</a>, and then moved to visual search with <a href="pub/TBME2016.pdf" target="_blank">artificial</a> stimuli or more <a href="pub/NER2015_2.pdf" target="_blank">realistic</a> stimuli of natural environments, <a href="pub/EMBC2016.pdf" target="_blank">speech perception</a>, and <a href="pub/NER2017.pdf" target="_blank">face recognition</a>. In all experiments, BCI-assisted groups were <em>significantly more accurate</em> than individuals and groups not assisted by the BCI.
                    </p>
                    <p align="center">
                        <img src="img/stim_plos.png" height="120px" width="200px" />
                        <img src="img/stim_bars.png" height="120px" width="200px" />
                        <img src="img/stim_bears.jpg" height="120px" width="200px" />
                        <img src="http://arma.sourceforge.net/chokepoint/figures/00001112.jpg" height="120px" width="200px" class="gray" />
                    </p>
                    <p>
                        While testing the cBCI, I was also interested in studying the group dynamics. For example, I have found that when people were allowed to exchange information during the experiment, their performance degraded significantly. Their confidence was uncorrelated with the objective accuracy, while the BCI was still able to recover good confidence estimates from their brain signals. See the <a href="http://dx.doi.org/10.1038/s41598-017-08265-7" target="blank">paper</a> for more details.
                    </p>
                    <p>
                        <img src="img/Cyborg.jpg" width="25%" align="right" hspace="15px" vspace="0px" />
                        I have also tested the collaborative BCI with <em>cyborg groups</em>, i.e., groups that include humans as well as algorithms that are autonomously able to make their own decisions and estimate their confidence. <a href="https://doi.org/10.1371/journal.pone.0212935" target="blank">This research</a> implemented effective human-machine teams for accurate face recognition in crowded environments, a critical task in surveillance. We used a <a href="https://github.com/ageitgey/face_recognition" target="blank">residual neural network</a> (ResNet) trained on millions of images to automatically classify each stimulus as containing or not the target face. Moreover, we gave the ResNet the ability of estimating its own confidence. Our results show that humans and machines working together achieve significantly better decisions than individually. We are now bringing this concept forward thanks to the funding provided jointly by the US DoD and UK MoD with the BARI project (see <a href="honors.html">Honors</a>).

                    <hr>
                    <h3>Bringing Brain-Computer Interfaces to People with Disabilities</h3>
                    <p>
                        Many BCIs developed in research labs are never translated into useful tools for people in needs. Pushed by the <a href="http://www.cybathlon.ethz.ch/en/" target="_blank">Cybathlon</a> competition, in 2015 I have teamed up with other PhD students and postdocs at the University of Essex to create the team <a href="http://essexbcis.uk/main/cybathlon/team/" target="_blank">Brainstormers</a>. Our aim was to develop a real-time BCI to allow <a href="http://essexbcis.uk/main/cybathlon/pilot/" target="blank">David</a> to control a videogame with his brain signals.
                    </p>
                    <p>
                        Our BCI was able to recognise four different mental tasks from the EEG signals and map them to the corresponding commands in the videogame. One of the most difficult parts to implement was the multi-class classification. For this task, we developed a multilayer ensemble (see <a href="pub/ECDA2015.pdf">this abstract</a> and the relative <a href="lectures/2015/ecda2015.pdf">presentation</a>). We then competed in the BCI race of the Cybathlon, the first international championship for parathletes assisted by technologies. Our student-led team ended up in the third position, bringing back a <strong>bronze medal</strong> to the UK. Most importantly, we gave hope to David and thousands of other people with severe disabilities.
                    </p>
                    <p>
                        The BrainStormers were supported by <a href="http://www.biosemi.com/" target="blank">Biosemi</a>, by the University of Essex, and by a crowdfunding campaign with which we raised more than &pound;3,000. Our work has attracted the interest of the media including <a href="http://www.itv.com/news/anglia/2016-05-18/essex-team-hope-to-storm-to-success-at-worlds-first-ever-cybathlon-competition/" target="_blank"><strong>ITV Anglia</strong></a> and <a href="https://cybathlon.wordpress.com/2015/01/29/brainstormers-and-bcis-on-your-marks-set-think/" target="_blank">Inside Cybathlon</a>.
                    </p>
                    <p align="center">
                        <img src="img/david_race.jpg" height="300px" />
                        <img src="img/brainstormers.jpg" height="300px" />
                    </p>
                    <hr>
                    <h3>Novel Human-Machine Interfaces</h3>
                    <img src="img/eyewink_logo.png" align="right" hspace="15px" width="20%">
                    <p>
                        EyeWink is a concept of a wearable device that allows a user to control his smartphone with eye winks. It detects muscular activity around the eyes by means of two electrodes placed on the forehead. The recorded signals are then processed by an electronic board to identify if the user had winked with either eye (see <a href="pub/CEEC2015.pdf">this paper</a>). Then, winks are transformed in commands and sent to the smartphone to be executed.
                    </p>
                    <p>
                        After pitching the idea at <a href="http://hackthebrain.uk/" target="_blank">HackTheBrain UK</a>, my team developed a first fully-working prototype in only 8 hours using <a href="http://openbci.com/community/hack-the-brain-uk-control-your-smartphone-by-winking/" target="_blank">OpenBCI</a>. After winning the hackathon, in November 2015 we started a <a href="https://click.hubbub.net/p/eye-wink" target="_blank">crowdfunding campaign</a> that allowed us to raise &pound;4,605 to support the development, and in December 2015 we co-founded <a href="https://beta.companieshouse.gov.uk/company/09907746" target="_blank">EyeWink Ltd</a>.
                    </p>
                    <p>
                        EyeWink has attracted a lot of interest from the media and the public. We have presented the technology in the London Science Museum twice, once in April 2015 within the exhibition "You have been upgraded", and once in April 2016 for the Science Museum Lates focused on neuroscience. We have appeared on
                        <a href="http://www.itv.com/news/anglia/2015-06-10/eye-phone-students-develop-new-tech-to-operate-mobiles/" target="_blank">ITV Anglia</a>,
                        <a href="http://www.popsci.com/control-your-projects-with-your-mind" target="_blank">Popular Science</a>,
                        <a href="http://motherboard.vice.com/read/brain-hackathon-hacking-brainwaves-to-extend-the-mind" target="_blank">Motherboard magazine</a>,
                        <a href="http://www.idgconnect.com/blog-abstract/9663/the-uk-hack-brain-event" target="_blank">IDG connect blog</a>,
                        <a href="http://www.idgconnect.com/abstract/10691/crowdsourcing-innovation-davide-valeriani-ana-matran-fernandez-eyewink" target="_blank">IDG connect innovation</a>,
                        the <a href="http://blogs.essex.ac.uk/essexdaily/2015/05/11/its-all-in-a-winkor-a-blink/" target="_blank">University of Essex daily news</a>, <a href="http://www.ilfattoquotidiano.it/2015/12/15/dottorato-a-londra-dopo-colloquio-via-skype-impensabile-in-italia-lavorare-al-mio-progetto-ultratecnologico/2302876/" target="_blank">Il Fatto Quotidiano</a>,
                        <a href="http://www.ilrestodelcarlino.it/reggio-emilia/eyewink-davide-valeriani-1.1537012" target="_blank">Resto del Carlino</a>,
                        <a href="http://gazzettadireggio.gelocal.it/reggio/cronaca/2015/11/24/news/lo-smartphone-lo-comandi-con-gli-occhi-1.12497579" target="_blank">Gazzetta di Reggio</a>,
                        and <a href="https://www.youtube.com/watch?v=403Uc5LlKE8" target="_blank">Radio Citt&agrave; Aperta</a>.
                    </p>
                    <p align="center">
                        <img src="img/eyewink_lates.jpg" height="300px" />
                        <iframe width="533" height="300" src="https://www.youtube.com/embed/1fRnrFB8ndI" frameborder="0" allowfullscreen align="right"></iframe>
                    </p>


                </div>
            </div>
        </div>
        <div id="push"></div>
    </div>
    <div id="footer">
        <div class="container">
            <p class="muted credit">Copyright &copy; 2020 Davide Valeriani. All rights reserved.</p>
        </div>
    </div>

    <!-- Latest compiled and minified JavaScript -->
    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-55902637-1', 'auto');
      ga('send', 'pageview');
  </script>
</body>
</html>
